---
title: "R Notebook: Master's theses work sheet"
output: html_notebook
---

# Master's theses work sheet introductional content

## Task

Pirikahu 2016 (<https://onlinelibrary.wiley.com/doi/10.1002/sim.6870>) esittelee Bayes-mallin, jolla voidaan estimoida riskitekijöiden merkitystä väestötutkimuksissa. Tähän liittyen voisi esim.

1)  implementoida menetelmän R:llä ja mahdollisesti myös Stanilla;

2)  laatia siitä yleiskäyttöisen ja hyvin dokumentoidun funktion johonkin meidän pakettiin;

3)  laatia workflow, joka esittelee menetelmän toimintaa ja eri puolia monipuolisesti, avoimesti saatavilla olevan esimerkkidatan avulla (ehdotuksia voidaan katsoa tarkemmin);

4)  analysoida sen toimintaa eri tilanteissa kuten pieni vs. iso näytemäärä; prioritiedon saatavuus;

5)  verrata ainakin yhteen vaihtoehtoiseen menetelmään;

6)  katsoa yleistyisikö helposti 2x2 kontingenssitauluista laajempiin (vaikutelmani on että tämä olisi suoraviivainen);

7)  mahdollisesti muita laajennuksia.

Julkaisu voi olla sinulle hankalalukuinen, mutta itse asiassa menetelmä on verraten suoraviivainen ja mm. analyyttisesti ratkeava.

## Introduction (my own text / info about data fron data source / CDC)

Starting development with data about '[Risk Factors for Cardiovascular Heart Disease](https://www.kaggle.com/datasets/thedevastator/exploring-risk-factors-for-cardiovascular-diseas/)' by Kuzak Dempsy.

```{r}
# Specify the file path.
file_path <- './heart_data.csv'

# Use read.csv() function to read the file
data <- read.csv(file_path)

# Print the first few rows of the data
head(data)
```

Goal is to implement a fully bayesien approach for calculatin population attributable risk factors or PAR for short. This approach is based on one outlined in paper 'Bayesian methods of confidence interval construction for the population attributable risk from cross-sectional studies' by Pirikahu.

I'll use the data to implement a single variable approach an later I will try to implement a multivariate version of this approach.

## Details of data

-   **age**: Age of the individual. (Integer)
-   **gender**: Gender of the individual. (String)
-   **height**: Height of the individual in centimeters. (Integer)
-   **weight**: Weight of the individual in kilograms. (Integer)
-   **ap_hi**: Systolic blood pressure reading. (Integer)
-   **ap_lo**: Diastolic blood pressure reading. (Integer)
-   **cholesterol**: Cholesterol level of the individual. (Integer)
-   **gluc**: Glucose level of the individual. (Integer)
-   **smoke**: Smoking status of the individual. (Boolean)
-   **alco**: Alcohol consumption status of the individual. (Boolean)
-   **active**: Physical activity level of the individual. (Boolean)
-   **cardio**: Presence or absence of cardiovascular disease. (Boolean)

[Source](https://www.kaggle.com/datasets/thedevastator/exploring-risk-factors-for-cardiovascular-diseas/)

According to [CDC](https://www.cdc.gov/chronicdisease/resources/publications/factsheets/heart-disease-stroke.htm) all of the attributes in the data are known risk factors or values that can be used to calculate the risk factor for cardio vascular disease. This means any value can be chosen to use for development. I'll choose physical activity: physical inactivity is a risk factor. This is a boolean value and this info will affect the priori. Choosing a continuous numerical value (like cholesterol) would varant a different type of priori

# From Pirikahu's paper

## Single variable approach.

Equation for populatin attributable risk: $PAR = P(D+) − P(D + |E−)$, where D(+∕−) denotes disease status and E(+∕−) exposure status to the risk factor.

### 2 × 2 contingency table

```{r}
# form a data frame for single variable analysis
cardioColumn <- data$cardio
activeColumn <- data$active

singleVarData <- data.frame(cardio = cardioColumn, active = activeColumn)

head(singleVarData)

```

```{r}
# Create a contingency table
contingencyTable <- table(singleVarData$cardio, singleVarData$active)

# Adding row and column labels
row_labels <- c("No Cardio vasc. disease, (D-)", "Cardio vasc.disease, (D+)")  
col_labels <- c("Not Physically Active, (E+)", "Physically Active, (E-)")   

# Applying row and column labels to the contingency table
dimnames(contingencyTable) <- list(row_labels, col_labels)

# Add row and column totals to the contingency table
contingencyTableWithTotals <- addmargins(contingencyTable)

# Print a title
cat("Contingency Table for Cardiovascular Disease and Physical Activity\n")

# Print the contingency table with totals
print(contingencyTableWithTotals)

```

|            | Not Physically Active | Physically Active | Total |
|------------|-----------------------|-------------------|-------|
| No Disease | a                     | b                 | a+b   |
| Disease    | c                     | d                 | c+d   |
| Total      | a+c                   | b+d               | n     |

$$
\hat{PAR} = \frac{a + c}{a + b + c + d} - \frac{c}{c + d}
$$

$(a, b, c, d) ∼ Multinomial(n, p11, p12, p21, p22)$

### A review of frequentist methods for confidence interval construction of the population attributable risk

The Delta method is a statistical method used to approximate the variance and standard error of a function of a random variable. In the context of estimating the Population Attributable Risk (PAR), the Delta method can be used to approximate the standard error of the PAR estimate, which can then be used to construct a confidence interval for the PAR.

#### Delta method

```{r}
# Define the observed counts
a <- 6378
b <- 28643
c <- 7361
d <- 27618

# Calculate the total count
N <- a + b + c + d

# Transform the counts into Poisson distributed data
a_poisson <- rpois(n = 1, lambda = a)
b_poisson <- rpois(n = 1, lambda = b)
c_poisson <- rpois(n = 1, lambda = c)
d_poisson <- rpois(n = 1, lambda = d)

# Calculate the PAR
par_hat <- ((a + c) / N) - (c / (c + d))

# Calculate the partial derivatives
dpardlambda1 <- (b_poisson + d_poisson) / N^2
dpardlambda2 <- -(a_poisson + c_poisson) / N^2
dpardlambda3 <- c_poisson / (c_poisson + d_poisson)^2 - (a_poisson + c_poisson) / N^2 + 1 / N - 1 / (c_poisson + d_poisson)
dpardlambda4 <- c_poisson / (c_poisson + d_poisson)^2 - (a_poisson + c_poisson) / N^2

# Calculate the variance of the PAR using the Delta method
var_par_hat <- dpardlambda1^2 * a + dpardlambda2^2 * b + dpardlambda3^2 * c + dpardlambda4^2 * d

# Calculate the standard error of the PAR
se_par_hat <- sqrt(var_par_hat)

# Calculate the 95% confidence interval for the PAR
conf_int_par <- c(par_hat - 1.96 * se_par_hat, par_hat + 1.96 * se_par_hat)

# Print the PAR estimate, its standard error, and its confidence interval
cat("PAR estimate: ", par_hat, "\n")
cat("Standard error of PAR estimate: ", se_par_hat, "\n")
cat("95% confidence interval for PAR estimate: (", conf_int_par[1], ", ", conf_int_par[2], ")\n")

```

### The bootstrap method

```{r}
# Define the observed counts
a <- 6378
b <- 28643
c <- 7361
d <- 27618

# Calculate the total count
N <- a + b + c + d

# Define the number of bootstrap samples
B <- 1000

# Initialize a vector to store the bootstrap PAR estimates
bootstrap_par <- numeric(B)

# Perform the bootstrap
for (i in 1:B) {
  # Sample from the multinomial distribution
  bootstrap_sample <- rmultinom(n = 1, size = N, prob = c(a/N, b/N, c/N, d/N))
  
  # Calculate the bootstrap PAR
  bootstrap_par[i] <- (sum(bootstrap_sample[1:2]) / N) - (bootstrap_sample[3] / sum(bootstrap_sample[3:4]))
}

# Calculate the bootstrap standard error
se_bootstrap_par <- sd(bootstrap_par)

# Calculate the bootstrap confidence interval
conf_int_bootstrap_par <- quantile(bootstrap_par, probs = c(0.025, 0.975))

# Print the bootstrap PAR estimate, its standard error, and its confidence interval
cat("Bootstrap PAR estimate: ", mean(bootstrap_par), "\n")
cat("Bootstrap standard error of PAR estimate: ", se_bootstrap_par, "\n")
cat("Bootstrap 95% confidence interval for PAR estimate: (", conf_int_bootstrap_par[1], ", ", conf_int_bootstrap_par[2], ")\n")
```

### The jackknife method

```{r}
# Define the observed counts
a <- 6378
b <- 28643
c <- 7361
d <- 27618

# Calculate the total count
N <- a + b + c + d

# Initialize a vector to store the jackknife PAR estimates
jackknife_par <- numeric(N)

# Perform the jackknife
for (i in 1:N) {
  # Create the jackknife sample
  if (i <= a) {
    jackknife_sample <- c(a - 1, b, c, d)
  } else if (i > a & i <= a + b) {
    jackknife_sample <- c(a, b - 1, c, d)
  } else if (i > a + b & i <= a + b + c) {
    jackknife_sample <- c(a, b, c - 1, d)
  } else if (i > a + b + c) {
    jackknife_sample <- c(a, b, c, d - 1)
  }
  
  # Calculate the jackknife PAR
  jackknife_par[i] <- (sum(jackknife_sample[1:2]) / N) - (jackknife_sample[3] / sum(jackknife_sample[3:4]))
}

# Calculate the jackknife standard error
se_jackknife_par <- sqrt((N - 1) / N * sum((jackknife_par - mean(jackknife_par))^2))

# Calculate the 95% confidence interval for the PAR
conf_int_jackknife_par <- c(mean(jackknife_par) - 1.96 * se_jackknife_par, mean(jackknife_par) + 1.96 * se_jackknife_par)

# Print the jackknife PAR estimate, its standard error, and its confidence interval
cat("Jackknife PAR estimate: ", mean(jackknife_par), "\n")
cat("Jackknife standard error of PAR estimate: ", se_jackknife_par, "\n")
cat("95% confidence interval for PAR estimate: (", conf_int_jackknife_par[1], ", ", conf_int_jackknife_par[2], ")\n")

```

## The Bayesian method

### Basic

```{r}

# Load the necessary package
library(MCMCpack)

# likelyhood function
calculate_PAR <- function(theta) {
  # Extract the parameters
  p11 <- theta[1]
  p12 <- theta[2]
  p21 <- theta[3]
  p22 <- theta[4]

  # Calculate Pd
  Pd <- (p11 + p21) / (p11 + p12 + p21 + p22)

  # Calculate and return PAR
  PAR <- Pd - p21 / (p21 + p22)
  return(PAR)
}

# Define the observed data
a <- 6378
b <- 28643
c <- 7361
d <- 27618
x <- c(a, b, c, d)  # Replace a, b, c, d with your actual data

# Define the prior parameters for the Dirichlet distribution
prior <- c(1, 1, 1, 1)

# Generate N samples from the posterior distribution
N <- 10000
posterior_samples <- rdirichlet(N, x + prior)

# Calculate the PAR for each sample
# Replace `calculate_PAR` with your actual function for calculating PAR
PAR_samples <- apply(posterior_samples, 1, calculate_PAR)

plot(PAR_samples)

```

where - `p11`: The probability of an individual being exposed and having the disease. - `p12`: The probability of an individual being exposed and not having the disease. - `p21`: The probability of an individual not being exposed and having the disease. - `p22`: The probability of an individual not being exposed and not having the disease. \### Analyze

```{r}
# Compute posterior mean
posterior_mean <- mean(PAR_samples)
print(paste("Posterior mean: ", posterior_mean))

# Compute 95% credible interval
credible_interval <- quantile(PAR_samples, c(0.025, 0.975))
print(paste("95% credible interval: ", credible_interval))

# Plot the distribution of PAR samples
hist(PAR_samples, main = "Posterior distribution of PAR", xlab = "PAR", breaks = 50)

# Assess model fit
# This can be done in many ways, depending on the specifics of your model
# One common method is to compare the observed data to the posterior predictive distribution
posterior_predictive_samples <- rdirichlet(N, posterior_samples)
pp_check <- mean(posterior_predictive_samples == x)  # proportion of samples where predicted data matches observed data
print(paste("Posterior predictive check: ", pp_check))
```

### Stan

Some decription about stan

```{r}
library(rstan)
# STAN model for the Dirichlet distribution
stan_model <- "
data {
  int<lower=0> N;  // number of observations
  int<lower=0> x[N];  // observed data
  vector[N] prior;  // prior parameters
}

parameters {
  simplex[N] theta;  // parameters of the Dirichlet distribution
}

model {
  target += dirichlet_lpdf(theta | prior);
  target += multinomial_lpmf(x | theta);
}

generated quantities {
  real PAR = (theta[1] + theta[3]) / sum(theta) - theta[3] / (theta[3] + theta[4]);
}
"

# Compile the STAN model
stan_fit <- stan(model_code = stan_model, data = list(N = 4, x = c(a, b, c, d), prior = c(1, 1, 1, 1)), iter = 10000, chains = 4)

# Extract the samples
posterior_samples <- extract(stan_fit, "PAR")$PAR

# Calculate the 95% credible interval
credible_interval <- quantile(posterior_samples, c(0.025, 0.975))

# Print the credible interval
print(credible_interval)

```

### Prior

Bayesian approaches allow us to consider not only the data but also our prior beliefs about the problem. Under this framework, an appropriate prior is selected to represent our beliefs concerning the model parameter 𝜽, which is then updated based on the observed data x. For convenience, we make use of the fact that the conjugate prior for a multinomial distribution is the Dirichlet distribution. We select our prior p(𝜽) to be Dirichlet(1, 1, 1, 1), which is often considered as a standard reference prior.

Given a set of observations $x$, the posterior distribution of $\theta$ is a Dirichlet distribution with parameters $a + 1$, $b + 1$, $c + 1$, and $d + 1$:

$$
\theta | x \sim \text{Dirichlet}(a + 1, b + 1, c + 1, d + 1)
$$

### Compare priors

### Compare data sizes

## Multivariate model

```{r}
library(dagitty)

dag_1 <- dagitty('dag {
  "active" -> "over weight"
  "over weight" -> "cardio"
  "smoke" -> "cardio"
  "active" -> "cardio"
}')

plot(dag_1)
```

5th chapter in statistical rethinking -\> how to build a model for multivariate analysis
